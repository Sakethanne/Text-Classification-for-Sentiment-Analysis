{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sakethanne/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sakethanne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import nltk          # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet data\n",
    "nltk.download('stopwords')   # Download StopWords data\n",
    "import re            # Regular expressions for text processing\n",
    "from bs4 import BeautifulSoup  # For HTML parsing\n",
    "import warnings      # To handle warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings for the remainder of the code\n",
    "warnings.filterwarnings(\"default\")  # Set warnings back to default behavior\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/sakethanne/anaconda3/lib/python3.11/site-packages (0.0.2)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sakethanne/anaconda3/lib/python3.11/site-packages (from bs4) (4.12.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sakethanne/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.4)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_2151/1858904372.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  full_data = pd.read_csv(\"amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n",
      "Skipping line 20773: expected 15 fields, saw 22\n",
      "Skipping line 39834: expected 15 fields, saw 22\n",
      "Skipping line 52957: expected 15 fields, saw 22\n",
      "Skipping line 54540: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 80276: expected 15 fields, saw 22\n",
      "Skipping line 96168: expected 15 fields, saw 22\n",
      "Skipping line 96866: expected 15 fields, saw 22\n",
      "Skipping line 98175: expected 15 fields, saw 22\n",
      "Skipping line 112539: expected 15 fields, saw 22\n",
      "Skipping line 119377: expected 15 fields, saw 22\n",
      "Skipping line 120065: expected 15 fields, saw 22\n",
      "Skipping line 124703: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 134024: expected 15 fields, saw 22\n",
      "Skipping line 153938: expected 15 fields, saw 22\n",
      "Skipping line 156225: expected 15 fields, saw 22\n",
      "Skipping line 168603: expected 15 fields, saw 22\n",
      "Skipping line 187002: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 200397: expected 15 fields, saw 22\n",
      "Skipping line 203809: expected 15 fields, saw 22\n",
      "Skipping line 207680: expected 15 fields, saw 22\n",
      "Skipping line 223421: expected 15 fields, saw 22\n",
      "Skipping line 244032: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 270329: expected 15 fields, saw 22\n",
      "Skipping line 276484: expected 15 fields, saw 22\n",
      "Skipping line 304755: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 379449: expected 15 fields, saw 22\n",
      "Skipping line 386191: expected 15 fields, saw 22\n",
      "Skipping line 391811: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 414348: expected 15 fields, saw 22\n",
      "Skipping line 414773: expected 15 fields, saw 22\n",
      "Skipping line 417572: expected 15 fields, saw 22\n",
      "Skipping line 419496: expected 15 fields, saw 22\n",
      "Skipping line 430528: expected 15 fields, saw 22\n",
      "Skipping line 442230: expected 15 fields, saw 22\n",
      "Skipping line 450931: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 465377: expected 15 fields, saw 22\n",
      "Skipping line 467685: expected 15 fields, saw 22\n",
      "Skipping line 485055: expected 15 fields, saw 22\n",
      "Skipping line 487220: expected 15 fields, saw 22\n",
      "Skipping line 496076: expected 15 fields, saw 22\n",
      "Skipping line 512269: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 529505: expected 15 fields, saw 22\n",
      "Skipping line 531286: expected 15 fields, saw 22\n",
      "Skipping line 535424: expected 15 fields, saw 22\n",
      "Skipping line 569898: expected 15 fields, saw 22\n",
      "Skipping line 586293: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 593880: expected 15 fields, saw 22\n",
      "Skipping line 599274: expected 15 fields, saw 22\n",
      "Skipping line 607961: expected 15 fields, saw 22\n",
      "Skipping line 612413: expected 15 fields, saw 22\n",
      "Skipping line 615913: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 677580: expected 15 fields, saw 22\n",
      "Skipping line 687191: expected 15 fields, saw 22\n",
      "Skipping line 710819: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 728692: expected 15 fields, saw 22\n",
      "Skipping line 730216: expected 15 fields, saw 22\n",
      "Skipping line 758397: expected 15 fields, saw 22\n",
      "Skipping line 760061: expected 15 fields, saw 22\n",
      "Skipping line 768935: expected 15 fields, saw 22\n",
      "Skipping line 769483: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 822725: expected 15 fields, saw 22\n",
      "Skipping line 823621: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 857041: expected 15 fields, saw 22\n",
      "Skipping line 857320: expected 15 fields, saw 22\n",
      "Skipping line 858565: expected 15 fields, saw 22\n",
      "Skipping line 860629: expected 15 fields, saw 22\n",
      "Skipping line 864033: expected 15 fields, saw 22\n",
      "Skipping line 868673: expected 15 fields, saw 22\n",
      "Skipping line 869189: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 938605: expected 15 fields, saw 22\n",
      "Skipping line 940100: expected 15 fields, saw 22\n",
      "Skipping line 975137: expected 15 fields, saw 22\n",
      "Skipping line 976314: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 985597: expected 15 fields, saw 22\n",
      "Skipping line 990873: expected 15 fields, saw 22\n",
      "Skipping line 991806: expected 15 fields, saw 22\n",
      "Skipping line 1019808: expected 15 fields, saw 22\n",
      "Skipping line 1021526: expected 15 fields, saw 22\n",
      "Skipping line 1023905: expected 15 fields, saw 22\n",
      "Skipping line 1044207: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1084683: expected 15 fields, saw 22\n",
      "Skipping line 1093288: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1136430: expected 15 fields, saw 22\n",
      "Skipping line 1139815: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1179821: expected 15 fields, saw 22\n",
      "Skipping line 1195351: expected 15 fields, saw 22\n",
      "Skipping line 1202007: expected 15 fields, saw 22\n",
      "Skipping line 1224868: expected 15 fields, saw 22\n",
      "Skipping line 1232490: expected 15 fields, saw 22\n",
      "Skipping line 1238697: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1258654: expected 15 fields, saw 22\n",
      "Skipping line 1279948: expected 15 fields, saw 22\n",
      "Skipping line 1294360: expected 15 fields, saw 22\n",
      "Skipping line 1302240: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1413654: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1687095: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1805966: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1892134: expected 15 fields, saw 22\n",
      "\n",
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_2151/1858904372.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_data = pd.read_csv(\"amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the tsv file as a Pandas frame\n",
    "full_data = pd.read_csv(\"amazon_reviews_us_Office_Products_v1_00.tsv\", delimiter='\\t', encoding='utf-8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>US</td>\n",
       "      <td>53005790</td>\n",
       "      <td>RLI7EI10S7SN0</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great value! A must if you hate to carry thing...</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "      <td>1998-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>US</td>\n",
       "      <td>52188548</td>\n",
       "      <td>R1F3SRK9MHE6A3</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Attaches the Palm Pilot like an appendage</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "      <td>1998-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>US</td>\n",
       "      <td>52090046</td>\n",
       "      <td>R23V0C4NRJL8EM</td>\n",
       "      <td>0807865001</td>\n",
       "      <td>307284585</td>\n",
       "      <td>Gods and Heroes of Ancient Greece</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Excellent information, pictures and stories, I...</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "      <td>1998-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>US</td>\n",
       "      <td>52503173</td>\n",
       "      <td>R13ZAE1ATEUC1T</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>class text</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "      <td>1998-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>US</td>\n",
       "      <td>52585611</td>\n",
       "      <td>RE8J5O2GY04NN</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Microsoft's Finest</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "      <td>1998-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640254 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1                US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2                US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3                US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4                US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "2640249          US     53005790   RLI7EI10S7SN0  B00000DM9M       223408988   \n",
       "2640250          US     52188548  R1F3SRK9MHE6A3  B00000DM9M       223408988   \n",
       "2640251          US     52090046  R23V0C4NRJL8EM  0807865001       307284585   \n",
       "2640252          US     52503173  R13ZAE1ATEUC1T  1572313188       870359649   \n",
       "2640253          US     52585611   RE8J5O2GY04NN  1572313188       870359649   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0           Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1                Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2        Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3        AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4        Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "...                                                    ...              ...   \n",
       "2640249                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640250                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640251                  Gods and Heroes of Ancient Greece  Office Products   \n",
       "2640252  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "2640253  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "\n",
       "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                 5            0.0          0.0    N                 Y   \n",
       "1                 5            0.0          1.0    N                 Y   \n",
       "2                 5            0.0          0.0    N                 Y   \n",
       "3                 1            2.0          3.0    N                 Y   \n",
       "4                 4            0.0          0.0    N                 Y   \n",
       "...             ...            ...          ...  ...               ...   \n",
       "2640249           4           26.0         26.0    N                 N   \n",
       "2640250           4           18.0         18.0    N                 N   \n",
       "2640251           4            9.0         16.0    N                 N   \n",
       "2640252           5            0.0          0.0    N                 N   \n",
       "2640253           5            0.0          0.0    N                 N   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1        Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                            but I am sure I will like it.   \n",
       "3        and the shredder was dirty and the bin was par...   \n",
       "4                                               Four Stars   \n",
       "...                                                    ...   \n",
       "2640249  Great value! A must if you hate to carry thing...   \n",
       "2640250          Attaches the Palm Pilot like an appendage   \n",
       "2640251  Excellent information, pictures and stories, I...   \n",
       "2640252                                         class text   \n",
       "2640253                                 Microsoft's Finest   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                                           Great product.  2015-08-31  \n",
       "1        What's to say about this commodity item except...  2015-08-31  \n",
       "2          Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3        Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                          Gorgeous colors and easy to use  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "2640249  I can't live anymore whithout my Palm III. But...  1998-12-07  \n",
       "2640250  Although the Palm Pilot is thin and compact it...  1998-11-30  \n",
       "2640251  This book had a lot of great content without b...  1998-10-15  \n",
       "2640252  I am teaching a course in Excel and am using t...  1998-08-22  \n",
       "2640253  A very comprehensive layout of exactly how Vis...  1998-07-15  \n",
       "\n",
       "[2640254 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data frame that contains the entire dataset from the tsv file\n",
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Sample Reviews:\n",
      "                                               review_body  star_rating  \\\n",
      "1293158  So cute, so simple. Great for a quick thank yo...          5.0   \n",
      "2109397  Looks like a real good stapler, small sturdy, ...          2.0   \n",
      "2202452  These folders are making a huge positive diffe...          5.0   \n",
      "\n",
      "                            review_headline  \n",
      "1293158  Great for a quick thank you letter  \n",
      "2109397                         ACCO Brands  \n",
      "2202452   So much nicer than simple folders  \n",
      "====================================================\n",
      "\n",
      "Ratings Statistics:\n",
      "1.0     306979\n",
      "2.0     138384\n",
      "3.0     193691\n",
      "4.0     418371\n",
      "5.0    1582812\n",
      "Name: star_rating, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_2151/222344944.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['star_rating'] = pd.to_numeric(data['star_rating'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Keep only the Reviews and Ratings fields\n",
    "data = full_data[['review_body', 'star_rating', 'review_headline']]\n",
    "\n",
    "# Convert 'star_rating' to numeric values\n",
    "data['star_rating'] = pd.to_numeric(data['star_rating'], errors='coerce')\n",
    "\n",
    "# Display three sample reviews along with ratings\n",
    "sample_reviews = data.sample(3)\n",
    "print(\"====================================================\")\n",
    "print(\"Sample Reviews:\")\n",
    "print(sample_reviews)\n",
    "\n",
    "# Report statistics of the ratings\n",
    "ratings_statistics = data['star_rating'].value_counts().sort_index()\n",
    "print(\"====================================================\")\n",
    "print(\"\\nRatings Statistics:\")\n",
    "print(ratings_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_2151/742703440.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentiment'] = data['star_rating'].apply(lambda x: 1 if x > 3 else 0 if x <= 2 else None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>980067</th>\n",
       "      <td>Yes they're thin, but they're sturdy and do th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Thin but Sturdy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655902</th>\n",
       "      <td>order came in just fine, would order again</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Name Badge magnets</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249308</th>\n",
       "      <td>exactly as it appears great for work</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190006</th>\n",
       "      <td>I just set this up in my classroom. It is actu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great product!</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935188</th>\n",
       "      <td>good product</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553480</th>\n",
       "      <td>Product does work, but requires multiple appli...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Product is better for wood furniture that is l...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378271</th>\n",
       "      <td>This is  not functioning i did all the seller ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Not working</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842986</th>\n",
       "      <td>This calendar would have been great if it hadn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bad product</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657727</th>\n",
       "      <td>This is a fake! Copy of a native.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>One Star</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227211</th>\n",
       "      <td>The voyage 200 is no help I could not use it b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Godschild</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating  \\\n",
       "980067   Yes they're thin, but they're sturdy and do th...          5.0   \n",
       "655902          order came in just fine, would order again          5.0   \n",
       "1249308               exactly as it appears great for work          4.0   \n",
       "2190006  I just set this up in my classroom. It is actu...          5.0   \n",
       "935188                                        good product          5.0   \n",
       "...                                                    ...          ...   \n",
       "553480   Product does work, but requires multiple appli...          1.0   \n",
       "1378271  This is  not functioning i did all the seller ...          1.0   \n",
       "1842986  This calendar would have been great if it hadn...          1.0   \n",
       "657727                   This is a fake! Copy of a native.          1.0   \n",
       "2227211  The voyage 200 is no help I could not use it b...          1.0   \n",
       "\n",
       "                                           review_headline  sentiment  \n",
       "980067                                     Thin but Sturdy        1.0  \n",
       "655902                                  Name Badge magnets        1.0  \n",
       "1249308                                         Four Stars        1.0  \n",
       "2190006                                     Great product!        1.0  \n",
       "935188                                          Five Stars        1.0  \n",
       "...                                                    ...        ...  \n",
       "553480   Product is better for wood furniture that is l...        0.0  \n",
       "1378271                                        Not working        0.0  \n",
       "1842986                                        Bad product        0.0  \n",
       "657727                                            One Star        0.0  \n",
       "2227211                                          Godschild        0.0  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create binary labels for sentiment analysis\n",
    "data['sentiment'] = data['star_rating'].apply(lambda x: 1 if x > 3 else 0 if x <= 2 else None)\n",
    "\n",
    "# Discard neutral reviews (rating 3)\n",
    "data = data.dropna(subset=['sentiment'])\n",
    "\n",
    "# Select 100,000 positive and 100,000 negative reviews\n",
    "positive_reviews = data[data['sentiment'] == 1].sample(20000, random_state=42)\n",
    "negative_reviews = data[data['sentiment'] == 0].sample(20000, random_state=42)\n",
    "\n",
    "# Concatenate positive and negative reviews\n",
    "selected_reviews = pd.concat([positive_reviews, negative_reviews])\n",
    "\n",
    "# Print the reviews that have been selected for further processing randomly\n",
    "selected_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Split the dataset into training and testing dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_reviews['review_body'],\n",
    "                                                    selected_reviews['sentiment'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2554960    I bought this printer from Staples. Staples ma...\n",
       "806282     good value and quick delivery.  No problems wi...\n",
       "442493                     Great price and original product.\n",
       "514835     It works, it's cute, not too much else to say....\n",
       "690043        ink was over one year past the use before date\n",
       "                                 ...                        \n",
       "1880369    I enjoy very much. help me in daily shredding ...\n",
       "1905026    This keeps my magic wand safe and gives me som...\n",
       "2547856    If you are using Windows 2000 DO NOT buy this ...\n",
       "1646091    We homeschool and my children have a daily lea...\n",
       "1519083    Love the wireless printing. Only problem is th...\n",
       "Name: review_body, Length: 32000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209501    I am at my wit's end and at this point I just ...\n",
       "1990644    This pen draws a solid, even, and confident li...\n",
       "1084939    I was so excited about this product when I fir...\n",
       "1590006    The HP printer was received in excellent condi...\n",
       "2558567    I have used zest for longer than I can remembe...\n",
       "                                 ...                        \n",
       "1932246    This printer is a great value for the price. T...\n",
       "1317813    Have not been able to hear a recording from th...\n",
       "868565                                Good reliable product.\n",
       "745594                                              Love it!\n",
       "418299     I put the batteries on the charge and they nev...\n",
       "Name: review_body, Length: 8000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2554960    1.0\n",
       "806282     1.0\n",
       "442493     1.0\n",
       "514835     1.0\n",
       "690043     0.0\n",
       "          ... \n",
       "1880369    1.0\n",
       "1905026    1.0\n",
       "2547856    0.0\n",
       "1646091    1.0\n",
       "1519083    1.0\n",
       "Name: sentiment, Length: 32000, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209501    0.0\n",
       "1990644    1.0\n",
       "1084939    0.0\n",
       "1590006    1.0\n",
       "2558567    0.0\n",
       "          ... \n",
       "1932246    1.0\n",
       "1317813    0.0\n",
       "868565     1.0\n",
       "745594     1.0\n",
       "418299     0.0\n",
       "Name: sentiment, Length: 8000, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/d1tzxzhj3fzbmswz4z7m_40w0000gn/T/ipykernel_2151/1539913846.py:35: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Length of Reviews (Before Cleaning): 315.87378125 characters\n",
      "Average Length of Reviews (After Cleaning): 298.18421875 characters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Define a simple contraction map\n",
    "CONTRACTION_MAP = {\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "}\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    for contraction, expansion in CONTRACTION_MAP.items():\n",
    "        text = re.sub(contraction, expansion, text)\n",
    "    return text\n",
    "\n",
    "# Preprocess the reviews\n",
    "def preprocess_reviews(reviews):\n",
    "    # Convert to lowercase and handle NaN values\n",
    "    reviews = reviews.apply(lambda x: str(x).lower() if pd.notna(x) else '')\n",
    "    \n",
    "    # Remove HTML and URLs\n",
    "    reviews = reviews.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "    reviews = reviews.apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "    # Remove non-alphabetical characters\n",
    "    reviews = reviews.apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    reviews = reviews.apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    # Perform contractions\n",
    "    reviews = reviews.apply(expand_contractions)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Preprocess the training set\n",
    "X_train_preprocessed = preprocess_reviews(X_train)\n",
    "\n",
    "# Print average length of reviews before and after cleaning\n",
    "avg_length_before = X_train.apply(lambda x: len(str(x))).mean()\n",
    "avg_length_after = X_train_preprocessed.apply(len).mean()\n",
    "print(f\"\\nAverage Length of Reviews (Before Cleaning): {avg_length_before} characters\")\n",
    "print(f\"Average Length of Reviews (After Cleaning): {avg_length_after} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "### -- remove the stop words\n",
    "### -- perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Review 1567770 Before Preprocessing:\n",
      "we had this exact machine for years it finally gave out when we tried to order a new one through our bank it they only had the newer models for we ordered this model new and it is great for of the cost very pleased\n",
      "\n",
      "Sample Review 1567770 After NLTK Preprocessing:\n",
      "exact machine year finally gave tried order new one bank newer model ordered model new great cost pleased\n",
      "\n",
      "Sample Review 1037094 Before Preprocessing:\n",
      "pen refill writes smooth at first but then starts to have problems with flow both refills worked like that so it was a problem with the design then there was the problem with the pen refill leaking inside the pen on the second refill alcohol hand sanitizer will loosen the in to clean your pen i am sticking with to original refills from now on\n",
      "\n",
      "Sample Review 1037094 After NLTK Preprocessing:\n",
      "pen refill writes smooth first start problem flow refill worked like problem design problem pen refill leaking inside pen second refill alcohol hand sanitizer loosen clean pen sticking original refill\n",
      "\n",
      "Sample Review 108826 Before Preprocessing:\n",
      "perfectly adequate for home use if i had more volume i would want something a little beefier but i am more than satisfied with this one\n",
      "\n",
      "Sample Review 108826 After NLTK Preprocessing:\n",
      "perfectly adequate home use volume would want something little beefier satisfied one\n",
      "\n",
      "Average Length of Reviews (Before NLTK Processing): 298.18421875 characters\n",
      "Average Length of Reviews (After NLTK Processing): 189.57253125 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK's stopwords and WordNet lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to remove stop words and perform lemmatization\n",
    "def preprocess_nltk(review):\n",
    "    if pd.notna(review):\n",
    "        words = nltk.word_tokenize(str(review).lower())  # Convert to lowercase\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Preprocess the training set using NLTK\n",
    "X_train_nltk_preprocessed = X_train_preprocessed.apply(preprocess_nltk)\n",
    "\n",
    "# Print three sample reviews before and after NLTK preprocessing\n",
    "sample_reviews_indices = X_train_preprocessed.sample(3).index\n",
    "\n",
    "for index in sample_reviews_indices:\n",
    "    print(f\"\\nSample Review {index} Before Preprocessing:\")\n",
    "    print(X_train_preprocessed.loc[index])\n",
    "\n",
    "    print(f\"\\nSample Review {index} After NLTK Preprocessing:\")\n",
    "    print(X_train_nltk_preprocessed.loc[index])\n",
    "\n",
    "# Print average length of reviews before and after NLTK processing\n",
    "avg_length_before_nltk = X_train_preprocessed.apply(len).mean()\n",
    "avg_length_after_nltk = X_train_nltk_preprocessed.apply(len).mean()\n",
    "print(f\"\\nAverage Length of Reviews (Before NLTK Processing): {avg_length_before_nltk} characters\")\n",
    "print(f\"Average Length of Reviews (After NLTK Processing): {avg_length_after_nltk} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train_tfidf: (32000, 10000)\n",
      "Shape of X_test_tfidf: (8000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # You can adjust the max_features parameter\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_nltk_preprocessed)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test.apply(preprocess_nltk))\n",
    "\n",
    "# Print the shape of the TF-IDF matrices\n",
    "print(f\"\\nShape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "Accuracy: 0.9260625\n",
      "Precision: 0.9460309143306261\n",
      "Recall: 0.9035405980232704\n",
      "F1-score: 0.9242976898956934\n",
      "\n",
      "Testing Set Metrics:\n",
      "Accuracy: 0.8325\n",
      "Precision: 0.8388241256969082\n",
      "Recall: 0.8246138515196811\n",
      "F1-score: 0.8316582914572863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize the Perceptron model\n",
    "perceptron_model = Perceptron(random_state=42)\n",
    "\n",
    "# Train the Perceptron model on the TF-IDF features\n",
    "perceptron_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred = perceptron_model.predict(X_train_tfidf)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = perceptron_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTraining Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_train}\")\n",
    "print(f\"Precision: {precision_train}\")\n",
    "print(f\"Recall: {recall_train}\")\n",
    "print(f\"F1-score: {f1_train}\")\n",
    "\n",
    "print(f\"\\nTesting Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(f\"Precision: {precision_test}\")\n",
    "print(f\"Recall: {recall_test}\")\n",
    "print(f\"F1-score: {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics (SVM):\n",
      "Accuracy: 0.9784375\n",
      "Precision: 0.9786581549630742\n",
      "Recall: 0.9781683973476792\n",
      "F1-score: 0.978413214866725\n",
      "\n",
      "Testing Set Metrics (SVM):\n",
      "Accuracy: 0.891625\n",
      "Precision: 0.8882309400444115\n",
      "Recall: 0.8968609865470852\n",
      "F1-score: 0.8925251022685012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Train the SVM model on the TF-IDF features\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred_svm = svm_model.predict(X_train_tfidf)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "accuracy_train_svm = accuracy_score(y_train, y_train_pred_svm)\n",
    "precision_train_svm = precision_score(y_train, y_train_pred_svm)\n",
    "recall_train_svm = recall_score(y_train, y_train_pred_svm)\n",
    "f1_train_svm = f1_score(y_train, y_train_pred_svm)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "accuracy_test_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "precision_test_svm = precision_score(y_test, y_test_pred_svm)\n",
    "recall_test_svm = recall_score(y_test, y_test_pred_svm)\n",
    "f1_test_svm = f1_score(y_test, y_test_pred_svm)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTraining Set Metrics (SVM):\")\n",
    "print(f\"Accuracy: {accuracy_train_svm}\")\n",
    "print(f\"Precision: {precision_train_svm}\")\n",
    "print(f\"Recall: {recall_train_svm}\")\n",
    "print(f\"F1-score: {f1_train_svm}\")\n",
    "\n",
    "print(f\"\\nTesting Set Metrics (SVM):\")\n",
    "print(f\"Accuracy: {accuracy_test_svm}\")\n",
    "print(f\"Precision: {precision_test_svm}\")\n",
    "print(f\"Recall: {recall_test_svm}\")\n",
    "print(f\"F1-score: {f1_test_svm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics (Logistic Regression):\n",
      "Accuracy: 0.90859375\n",
      "Precision: 0.9146612483332275\n",
      "Recall: 0.9011009633429251\n",
      "F1-score: 0.9078304710887034\n",
      "\n",
      "Testing Set Metrics (Logistic Regression):\n",
      "Accuracy: 0.884625\n",
      "Precision: 0.8813224771773994\n",
      "Recall: 0.8898854010961634\n",
      "F1-score: 0.8855832403619686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model on the TF-IDF features\n",
    "logreg_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred_logreg = logreg_model.predict(X_train_tfidf)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred_logreg = logreg_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "accuracy_train_logreg = accuracy_score(y_train, y_train_pred_logreg)\n",
    "precision_train_logreg = precision_score(y_train, y_train_pred_logreg)\n",
    "recall_train_logreg = recall_score(y_train, y_train_pred_logreg)\n",
    "f1_train_logreg = f1_score(y_train, y_train_pred_logreg)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "accuracy_test_logreg = accuracy_score(y_test, y_test_pred_logreg)\n",
    "precision_test_logreg = precision_score(y_test, y_test_pred_logreg)\n",
    "recall_test_logreg = recall_score(y_test, y_test_pred_logreg)\n",
    "f1_test_logreg = f1_score(y_test, y_test_pred_logreg)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTraining Set Metrics (Logistic Regression):\")\n",
    "print(f\"Accuracy: {accuracy_train_logreg}\")\n",
    "print(f\"Precision: {precision_train_logreg}\")\n",
    "print(f\"Recall: {recall_train_logreg}\")\n",
    "print(f\"F1-score: {f1_train_logreg}\")\n",
    "\n",
    "print(f\"\\nTesting Set Metrics (Logistic Regression):\")\n",
    "print(f\"Accuracy: {accuracy_test_logreg}\")\n",
    "print(f\"Precision: {precision_test_logreg}\")\n",
    "print(f\"Recall: {recall_test_logreg}\")\n",
    "print(f\"F1-score: {f1_test_logreg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics (Multinomial Naive Bayes):\n",
      "Accuracy: 0.88121875\n",
      "Precision: 0.8892225132562448\n",
      "Recall: 0.870699361941699\n",
      "F1-score: 0.8798634596542242\n",
      "\n",
      "Testing Set Metrics (Multinomial Naive Bayes):\n",
      "Accuracy: 0.86225\n",
      "Precision: 0.860932077342588\n",
      "Recall: 0.8652217239661186\n",
      "F1-score: 0.8630715705765408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train the Multinomial Naive Bayes model on the TF-IDF features\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on the training set\n",
    "y_train_pred_nb = nb_model.predict(X_train_tfidf)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "accuracy_train_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "precision_train_nb = precision_score(y_train, y_train_pred_nb)\n",
    "recall_train_nb = recall_score(y_train, y_train_pred_nb)\n",
    "f1_train_nb = f1_score(y_train, y_train_pred_nb)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "accuracy_test_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "precision_test_nb = precision_score(y_test, y_test_pred_nb)\n",
    "recall_test_nb = recall_score(y_test, y_test_pred_nb)\n",
    "f1_test_nb = f1_score(y_test, y_test_pred_nb)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTraining Set Metrics (Multinomial Naive Bayes):\")\n",
    "print(f\"Accuracy: {accuracy_train_nb}\")\n",
    "print(f\"Precision: {precision_train_nb}\")\n",
    "print(f\"Recall: {recall_train_nb}\")\n",
    "print(f\"F1-score: {f1_train_nb}\")\n",
    "\n",
    "print(f\"\\nTesting Set Metrics (Multinomial Naive Bayes):\")\n",
    "print(f\"Accuracy: {accuracy_test_nb}\")\n",
    "print(f\"Precision: {precision_test_nb}\")\n",
    "print(f\"Recall: {recall_test_nb}\")\n",
    "print(f\"F1-score: {f1_test_nb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
